{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Python and NumPy Concepts for Efficient Python Programming\n",
    "> CogWorks 2019 (Petar Griggs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dictionaries\n",
    "---\n",
    "As part of the Python course, you all should have had experience using [dictionaries](https://www.pythonlikeyoumeanit.com/Module2_EssentialsOfPython/DataStructures_II_Dictionaries.html). They are a powerful tool Python offers for when we have some form of data where key-value pairs are natural. This is a result of the speedy lookup that dictionaries have, thanks to a method known as \"hashing\".\n",
    "\n",
    "Put simply, when we create a dictionary, Python creates an contiguous array in memory. When we give the dictionary a key, it will run that key through a hashing algorithm that will point to a unique position in that array. Whatever value we provided that should be associated with our key will then be stored in the array at that position.\n",
    "\n",
    "So, when we want to find a value that corresponds to a specific key, we don't have to iterate over every slot in the array. We can simply run the hashing algorithm again to reproduce the hash, then retrieve whatever is stored at the appropriate position in memory.\n",
    "\n",
    "On average, this hashing process runs in what is known as $\\mathcal{O}(1)$, or constant, time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to understand just what this means. For a given algorithm, the time complexity is simply how long it takes to run that algorithm. We can characterize time complexity in a number of ways, but it is common to use either the average or worst-case runtime. Right now, we'll consider the average run-time, which we can denote by $\\mathcal{O}$.\n",
    "\n",
    "Say we want to sum up every element in a list of length $n$. To do so, we will need to 'hit' each element in the list once. So, our algorithm has $\\mathcal{O}(n)$ time complexity. Or, consider trying to find the minimum value in a 2D $m\\times n$ array. Our search algorithm will then run in $\\mathcal{O}(mn)$ time.\n",
    "\n",
    "With that said, the $\\mathcal{O}(1)$ time that hashing achieves is great. In fact, it's the best we can ever hope to achieve. Our dictionary can be arbitraily long and we can always find the exact value we want in the same amount of time! Meanwhile, finding the exact value in a list (without knowing its corresponding index) takes $\\mathcal{O}(n)$ time, as we may need to iterate over element before finding the desired value. This is why dictionaries in Python are so powerful!\n",
    "\n",
    "On a completely unrelated note, my personal favorite algorithm is \"Bogosort\", a sorting algorithm which has worst-case runtime of $\\mathcal{O}(\\infty)$ but best-case runtime of $\\mathcal{O}(1)$. How? It randomly shuffles the list and checks whether or not the result is sorted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Itertools\n",
    "---\n",
    "The itertools library provides a wide range of generator functions that enable us to efficiently work with looping in Python.\n",
    "\n",
    "You should look over the [itertool's documentation](https://docs.python.org/3/library/itertools.html) for a complete list of features. Take a moment to do so, then, using itertools, write up a function that takes in a string and returns the same string, with letters alternating between capital and lowercase. As a hint, you may want to use the zip function, or some variation of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "def alt_caps(string):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_caps(\"hello there, general kenobi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Indexing\n",
    "---\n",
    "Indexing into NumPy arrays comes in two forms: [basic](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/BasicIndexing.html#Basic-Indexing) and [advanced](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/AdvancedIndexing.html?highlight=indexing) indexing.\n",
    "\n",
    "Basic indexing is what most of you are likely pretty familiar with: indexing a single value in an array, or a slice of the array. Basic indexing will always return a *view* of the array, meaning that the value returned still belongs the the original array. This also means that **any update you make to this slice of your array will be reflected in the original array**. This behaviour can be the root of a lot of painful to find bugs, so it is important to be cautious when working with views.\n",
    "\n",
    "Take a moment to instantiate an array. Slice that array and perform an in-place update of the slice. Then observe the original array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike basic indexing, advanced indexing always returns a copy of the data. Advanced indexing can take a few forms, including indexing with a seperate array of values or a boolean array. Let's take a look at an example of indexing with another array:\n",
    "```python\n",
    ">>> arr = np.array([1, 2, 3, 4, 5])\n",
    ">>> inds = np.array([3, 4, 4, 0])\n",
    ">>> arr[inds]\n",
    "np.array([4, 5, 5, 1])\n",
    "```\n",
    "\n",
    "Advanced indexing also allows us to change an array given a condition. Say we want to zero out all negative entires in an array. This can be easily accomplished with advanced indexing:\n",
    "```python\n",
    ">>> arr = np.array([1, -2, -3, -4, 5])\n",
    ">>> arr < 0 # pass array into conditional statement to apply conditional elementwise, yielding boolean array\n",
    "array([False, True, True, True, False])\n",
    ">>> arr[arr < 0] # index with boolean array, returning the values in array satisfying condition; data is copied\n",
    "array([-2, -3, -4])\n",
    ">>> arr[arr < 0] = 0 # zero out negative values in-place, still copying data\n",
    ">>> arr\n",
    "array([1, 0, 0, 0, 5])\n",
    "```\n",
    "\n",
    "As mentioned, advanced indexing, will always return a copy of the data being indexed. While this avoids the previous issues with in-place operations on views, it can also be very expensive to copy data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider an example from statistics. The ubiquitous Normal distribution, shaped as a bell curve, can be described by two parameters: its mean (where the center of the curve lies) and its variance (how thick the curve is). The Multivariate Normal generalizes the Normal into arbitrary dimensions, and can be parametrized by a mean *vector* and a covariance *matrix* (where the diagonals are the variances in each dimension, and the off-diagonals are covariances between dimensions). Note that the covariance matrix should be symmetric: element $i,j$ should be equal to element $j,i$.\n",
    "\n",
    "Don't worry if all of this sounds new to you. We are just providing some context here.\n",
    "\n",
    "Say we want to draw samples from a Multivariate Normal where the covariance matrix has high variances (diagonals) and low covariances (off-diagonals). We want to find the mean variance and covariance, to get a general idea of how much each how the dimensions relate to one another. Unfortunately because our variances cut across our covariance matrix, we can't just call `np.mean` on a specific axis. But we can still do this using advanced indexing!\n",
    "\n",
    "Take some time to compute the means of the diagonal and off-diagonal entries in the array. You can construct a covariance matrix with:\n",
    "```python\n",
    "np.random.seed(1)\n",
    "S = np.random.rand(100, 100)\n",
    "S = np.dot(S, S.T) / 30 + 3 * np.eye(S.shape[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simulate draws from an appropriate Multivariate Normal distribution to confirm our answers! NumPy provides the `np.random.multivariate_normal` function for doing just this. We can inspect the docstring for this function by writing it out in a code cell and pressing Shift-Tab. Wow! Now we know not only what parameters we should pass in, but we also know what the output will look like.\n",
    "\n",
    "First, let's start by generating a few thousand random samples from a $100$-dimensional Multivariate Normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the mean variance of our sample. We can use NumPy's `np.var` function to find variance. But variance is measured in each dimension individually, so make sure that you apply the function over the correct axis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty close to the mean variance we calculated earlier, right? Let's do the same for our covariances now. NumPy also has a handy `np.cov` function we can take advantage of. Here, take note of the `rowvar` parameter. Did each dimension correspond to a row or a column in our sample?\n",
    "\n",
    "Compare the result of applying `np.cov` and finding the mean to the result for the mean covariance we found earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Vectorization\n",
    "---\n",
    "We are going to be doing a lot of computation over the next 4 weeks. Sadly this is one area where Python on its own fails pretty catastrophically, as a result of the niceties provided by being a dynamically typed language. So if we wanted to calculate the square of every number from 1 to 1000, we *could* write a for loop or list comprehension to do so. But if we did, we would be sitting at our computers for an uncomfortably long period of time.\n",
    "\n",
    "Thankfully NumPy and its \"[vectorization](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html)\" comes to our rescue. For operations like these, NumPy can kick out to highly optimized C code to perform blazing fast computations. As every element in a NumPy array has the same data type, no type hecking needs to occur at every element, as is necessary with pure-Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out a calculation used in statistical mechanics when modelling ferromagnetism as an example (you do not need to have any familiarity with either statistical mechanics or magnetism for this problem though!)\n",
    "\n",
    "Take a 2D array consisting of randomly distributed $\\pm1$s. We want to calculate the energy of this 2D array. In fancy math terms, the energy of a given state\n",
    "\n",
    "\\begin{equation}\n",
    "H = \\sum_{\\langle i\\;j\\rangle}ij\n",
    "\\end{equation}\n",
    "\n",
    "where $\\langle i\\;j\\rangle$ represents the pair of connected vertices $i$ and $j$.\n",
    "\n",
    "Really all we want to do is sum up the product of each pair of vertically and horizontally adjacent values in our 2D array. Taking a simple $2\\times2$ example,\n",
    "```python\n",
    "np.array([[1, -1],\n",
    "          [1,  1]])\n",
    "\n",
    "((1 * 1) + (-1 * 1)) + ((1 * -1) + (1 * 1)) = 0\n",
    "^^^^^^^^^^^^^^^^^^^^   ^^^^^^^^^^^^^^^^^^^^\n",
    "    Column Pairs             Row Pairs\n",
    "```\n",
    "Here, the total energy is $0$.\n",
    "\n",
    "Now, take some time to write up two functions, one that calculates the energy of the array using a for loop, and one using vectorization. Make sure that both your functions return the same value!\n",
    "\n",
    "You can use\n",
    "```python\n",
    "L = np.random.choice([-1, 1], (1000, 1000))\n",
    "```\n",
    "to create your 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.random.choice([-1, 1], (1000, 1000))\n",
    "\n",
    "def loop_hamilton(L):\n",
    "    pass\n",
    "    \n",
    "def vec_hamilton(L):\n",
    "    pass\n",
    "\n",
    "assert loop_hamilton(L) == vec_hamilton(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit loop_hamilton(L)\n",
    "%timeit vec_hamilton(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Broadcasting\n",
    "---\n",
    "One of NumPy's other amaxing features is that of [broadcasting](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/Broadcasting.html). Broadcasting allows us to efficiently perform computations on two arrays with different shapes. Well, not completly different. Broadcasting requires that **the trailing dimensions are aligned**. This means that if we take two array's shapes and line them up, one of two conditions must be satisfied for each overlapping dimension:\n",
    "    - the aligned dimensions have the same size\n",
    "    - one of the dimensions has a size of 1\n",
    "\n",
    "As an example, lets say we want to multiply two arrays together, with shapes $(5,2,8,1)$ and $(1,8,9)$. Then:\n",
    "```python\n",
    "    1 x 8 x 9\n",
    "5 x 2 x 8 x 1\n",
    "-------------\n",
    "5 x 2 x 8 x 9\n",
    "```\n",
    "\n",
    "However, lets say our second array was actually $(1,9,8)$. Then,\n",
    "```python\n",
    "    1 x 9 x 8\n",
    "5 x 2 x 8 x 1\n",
    "-------------\n",
    "5 x 2 x ? x 8 >> NOT BROADCAST COMPATABLE\n",
    "```\n",
    "\n",
    "Because the second-to-last dimensions of both arrays are not aligned, the two arrays are not broadcast-compatable. We may need to do some reshaping if we want our arrays to be broadcast-compatable, but we must also make sure that in doing so we are not changing the operations we are performing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the Kronecker product, leveraging NumPy's broadcasting. The Kronecker product has applications in high-dimensional probability and image processing, but for now, we'll just consider the operation itself. With $A$ and $B$ as $m\\times n$ and $p\\times q$ matrices, the Kronecker product can be computed by taking each element in $A$ and multiplying it with the whole matrix $B$. This yields a giant $mp\\times nq$ matrix!\n",
    "\n",
    "It's easiest to understand this operation by looking at an example:\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}0 & 1 \\\\ 2 & 3\\end{bmatrix}\\otimes\\begin{bmatrix}4 & 5 \\\\ 6 & 7\\end{bmatrix}=\\begin{bmatrix}0\\times\\begin{bmatrix}4 & 5 \\\\ 6 & 7\\end{bmatrix} & 1\\times\\begin{bmatrix}4 & 5 \\\\ 6 & 7\\end{bmatrix} \\\\ 2\\times\\begin{bmatrix}4 & 5 \\\\ 6 & 7\\end{bmatrix} & 3\\times\\begin{bmatrix}4 & 5 \\\\ 6 & 7\\end{bmatrix}\\end{bmatrix}=\\begin{bmatrix}0\\times4 & 0\\times5 & 1\\times4 & 1\\times5 \\\\ 0\\times6 & 0\\times7 & 1\\times6 & 1\\times7 \\\\ 2\\times4 & 2\\times5 & 3\\times4 & 3\\times5 \\\\ 2\\times6 & 2\\times7 & 3\\times6 & 3\\times7\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "There are multiple ways to put the Kronecker product into code - including using broadcasting. NumPy even has its own implementation, but let's try to put together our own.\n",
    "\n",
    "First implement the Kronecker product using for loops, then try to implement it by leveraging broadcasting. Hint: for broadcasting, you will need to leverage np.newaxis or reshape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time both your implementation and NumPy's using the `%timeit` line-magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does our custom broadcasted Kronecker product work though? Well, when one of the dimensions is $1$, that dimension is \"stretched\" to match that of the other array. So, while we reshape $A$ to be of shape $(m,1,n,1)$ and $B$ to be of shape $(1,p,1,q)$, both matrices will be \"streched\" to be of shape $(m,p,n,q)$. The two matrices are then able to be multipled element-wise with each other. This allows us to efficiently calculate the product between every pair of elements in $A$ and $B$, which is all the Kronecker product really is! It also results in the corresponding dimensions of each array appearing consecutively in the out array, which means when we reshape the result, we will get the proper $(m\\times p, n\\times q)$ array as the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Tracebacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're all too familiar with seeing errors (except me, cause I ***always*** right perfect code on my first go). Let's take a look at the traceback from a Jupyter notebook and figure out what exactly Python doesn't like about our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [1, 2, 3]\n",
    "ls[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Jupyter notebooks, a traceback will appear if at any point during the exectuion of a specific cell, an error is hit. In the example above, the traceback tells us that there is an issue with the line `ls[3]` (indicated by the arrow pointing to that line in the code snippet). The traceback also tells us what exactly is the problem: here we have an `IndexError`. Sometimes the error will come with additional information, helping us identify specifically what the issue is. In this case, the traceback tells us that `list index out of range`. This example is pretty self-explanatory: we are trying to index into a list, but our index goes beyond the largest index valid for the list (which for a zero-based indexing system is the number of elements minus 1).\n",
    "\n",
    "Let's take a look at a more complicated example (here, mg is a library much like NumPy and Tensor is a class much like the ndarray):\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-35-7bea71fbe6ff> in <module>\n",
    "----> 1 mg.Tensor(np.array([True]))\n",
    "\n",
    "~/anaconda3/lib/python3.7/site-packages/mygrad/tensor_base.py in __init__(self, x, dtype, constant, _scalar_only, _creator)\n",
    "    135         else:\n",
    "    136             self.data = np.asarray(x, dtype=dtype)\n",
    "--> 137             self._check_valid_dtype(self.data.dtype)\n",
    "    138 \n",
    "    139         self.grad = None  # type: Union[None, np.ndarray]\n",
    "\n",
    "~/anaconda3/lib/python3.7/site-packages/mygrad/tensor_base.py in _check_valid_dtype(dtype)\n",
    "    149     def _check_valid_dtype(dtype):\n",
    "    150         if not np.issubdtype(dtype, np.number):\n",
    "--> 151             raise TypeError(\"Tensor data must be a numeric type, received {}\".format(dtype))\n",
    "    152 \n",
    "    153     @classmethod\n",
    "\n",
    "TypeError: Tensor data must be a numeric type, received bool\n",
    "```\n",
    "\n",
    "The first thing we should notice is the last line, where the traceback tells us that there is a `TypeError` (so somewhere in our code we are trying to use the wrong type of object). Additionally, we can see that `Tensor data must be a numeric type, received bool`: so, somewhere where we should be using a number, we passed in boolean. Looking toward the beginning of the traceback, we can see our error occured with `mg.Tensor(np.array([True]))`. At this point, we have all we need to resolve this bug: we can deduce that we should not have passed `[True]` to `mg.Tensor`, and should now change our code to instead pass in a numerical value.\n",
    "\n",
    "The rest of the traceback shows the path our code took through the `mg` library. This is extremely useful for finding bugs in our own projects–we can then hunt down exactly where an error is being thrown in our whole code base and better understand why that error is thrown.\n",
    "\n",
    "Below are several code cells with buggy code. Run each cell to see the traceback. Then fix the issue with the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_n(n):\n",
    "    \"\"\" Computes n % (1, ..., n-1) for given n\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number to compute modulo up to\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[int]\n",
    "        List of integers resulting from modulo\n",
    "    \"\"\"\n",
    "    return [n % i for i in range(n)]\n",
    "\n",
    "mod_n(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repl_rev(text, repl=None):\n",
    "    \"\"\" Prints a given string in reverse, with specified characters replaced\n",
    "    \n",
    "    Paremeters\n",
    "    ----------\n",
    "    text : str\n",
    "        Text to be reverse\n",
    "    \n",
    "    repl : Dict[str:str]\n",
    "        Dictionary with charcaters to be replaced (keys) and caharcters to replace with (values)\n",
    "    \"\"\"\n",
    "    return \"\".join(char for char in text[::-1] if char not in repl else repl[char])\n",
    "\n",
    "repl_rev(\"according to all known laws of aviation, there is no way that a bee should be able to fly.\", repl={\"a\":\"B\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_square(arr):\n",
    "    \"\"\" Squares all values in array of consecutive positive integers, negating all odd values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray, shape=(N,)\n",
    "        1D array of N conseuctive positive integers\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        NumPy array of squared values, with odds negated\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> arr = np.array([1, 2, 3])\n",
    "    >>> sign_square(arr)\n",
    "    array([-1, 4, -9])\n",
    "    \"\"\"\n",
    "    if arr[0] % 2 == 0:\n",
    "        arr[::2] **= 2\n",
    "        arr[1::2] **= 2\n",
    "        arr[1::2] *= -1\n",
    "    else: \n",
    "        arr[1::2] **= 2\n",
    "        arr[::2] **= 2\n",
    "        arr[::2] *= -1\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def test_sign_square():\n",
    "    \"\"\" Tests if sign_square function works \"\"\"\n",
    "    arr = np.arange(10)\n",
    "    \n",
    "    sgn_sq_arr = sign_square(arr)\n",
    "    ls_comp_arr = [i ** 2 if i % 2 == 0 else -(i ** 2) for i in arr]\n",
    "    \n",
    "    eq_arr = sgn_sq_arr == np.array(ls_comp_arr)\n",
    "    \n",
    "    assert np.all(eq_arr), \"Not all elements are equal\"\n",
    "\n",
    "test_sign_square()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Another Advanced Indexing Exercise\n",
    "---\n",
    "Let's consider an example that comes up often when working with certain data sets. A \"sparse\" matrix is one in which the vast majority of elements are $0$s. In contrast, a \"dense\" matrix has relatively few $0$s.\n",
    "\n",
    "Matrix multiplication is already a very expensive operation, running in $\\mathcal{O}(n^3)$ time for an $n\\times n$ matrix. When we multiply dense matrices, there isn't too much we can do about this. But when we are multiplying sparse matrices, most of the operations we perform will include multiiplying with a $0$. This means that we are wasting quite a lot of compute on performing operations that won't affect our end result.\n",
    "\n",
    "There are plenty of efficient algorithms that handle sparse matrix operations, but let's try to develop our own using NumPy's advanced indexing! We will matrix multiply a sparse matrix by its transpose. First, try this naively, and see how long it takes to run. Then, remove all rows and columns where the only element is $0$ and try the matrix multiplication again. Don't worry about inserting rows and columns of $0$s in the output to recover the original output's shape, but you can check that the final non-zero outputs are the same by summing all the elements.\n",
    "\n",
    "You can use \n",
    "```python\n",
    "np.random.seed(1)\n",
    "S = np.zeros((10000, 1000))\n",
    "rows = np.random.randint(0, 10000, 100)\n",
    "cols = np.random.randint(0, 1000, 100)\n",
    "S[rows, cols] = np.random.randn(100)\n",
    "```\n",
    "to generate a sparse matrix (which also happens to use advanced indexing!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
