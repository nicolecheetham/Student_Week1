{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Mathematical Concepts for Fourier Transforms and Other Advanced Topics\n",
    ">Cogworks 2018 (Melanie Chen & Ryan Soklaski)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Summation Notation\n",
    "### 1.1 General Notation\n",
    "It is important that we are comfortable with reading and manipulating mathematical formulas that involve sums. In order to more efficiently denote sums, especially sums over long sequences, we can use summation notation, also known as sigma notation due to the use of the the Greek letter \"sigma\": $\\Sigma$. \n",
    "\n",
    "There are several elements involved in summation notation. First, consider a sequence of n numbers $x_0, x_1, \\cdots x_{n-1}$. We will start our index at 0, to remain in accordance with Python/NumPy's index system. $x_0$ is the first number in the sequence, $x_1$ is the second number in the sequence, and so forth, so $x_i$ is the general $i+1$ number in the sequence. We will utilize this $i$ index in our summation notation. Suppose we were to calculate the sum of all $n$ of these numbers in the sequence. We write that this sum is equivalent to \n",
    "\\begin{equation}\n",
    "\\sum_{i=0}^{n-1} x_i = x_0+x_1+ \\cdots x_{n-1}\n",
    "\\end{equation}\n",
    "Let's parse this equation. $\\Sigma$ is the summation sign, indicating that we are summing a sequence. $i$ is the index of summation that is being iterated over; $i$ is being used to subscript $x$. Then, we start our summation at 0 because the lower bound on our sum is set as $i=0$. The upper bound for our sum, or our stopping point is written as $n-1$, which is equivalent to writing $i=n-1$ on top of the $\\Sigma$ symbol. Then, $x_i$ indicates the quantitiy that we are summing. One way to think about this is in terms of a for-loop. This summation concept is equivalent to a for loop for all integers in the range from the lower bound to the upper bound, indexed into the sequence $x$. Then, the code for our previous sum is:\n",
    "\n",
    "```python\n",
    "total = 0\n",
    "x = [1, 3, -2, 0, 10]\n",
    "n = len(x)\n",
    "for i in range(n): # i = 0, 1, ..., n-1\n",
    "    total += x[i]\n",
    "    \n",
    "# this is equivalent to\n",
    "total = sum(x)\n",
    "```\n",
    "\n",
    "Let's run through a few more examples of summation notation. \n",
    "\n",
    "Example 1:\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n} i^2= 1 + 4+ \\cdots n^2 = \\frac{n(n+1)(2n+1)}{6}\n",
    "\\end{equation}\n",
    "\n",
    "```python\n",
    "sum(i**2 for i in range(1, n + 1))\n",
    "```\n",
    "\n",
    "Example 2:\n",
    "\\begin{equation}\n",
    "\\sum_{i=3}^{6} x_i^2 = x_3^2+x_4^2+x_5^2+x_6^2\n",
    "\\end{equation}\n",
    "\n",
    "```python\n",
    "sum(x[i]**2 for i in range(3, 7))\n",
    "```\n",
    "\n",
    "Example 3:\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{5} i^2-i = 0+2+6+12+20=40\n",
    "\\end{equation}\n",
    "\n",
    "```python\n",
    "sum((i**2 - i) for i in range(1, 6))\n",
    "```\n",
    "\n",
    "Then, we can also sum over several different sequences. Consider the sequences A and B, where A consists of m values $a_0, a_1, \\cdots, a_{m-1}$ and B contains n values $b_0, b_1, \\cdots b_{n-1}$. Then, we can calculate the sum of the product of each value in A with each value in B (a sum over $m\\times n$ products). Because the $i$ index only appears in association with A, and the $j$ index with B, we can group these summations. \n",
    "\\begin{equation}\n",
    "\\sum_{i=0}^{m-1} \\sum_{j=0}^{n-1} a_i \\cdot b_j = \\left(\\sum_{i=0}^{m-1} a_i\\right) \\cdot \\left(\\sum_{j=0}^{n-1} b_j\\right) = (a_0+a_1 + a_2 + \\cdots + a_{m-1}) \\cdot (b_0 + b_1 + b_2 + \\cdots b_{n-1})\n",
    "\\end{equation}\n",
    "\n",
    "```python\n",
    ">>> import numpy as np\n",
    ">>> A = np.random.rand(5)\n",
    ">>> B = np.random.rand(7)\n",
    ">>> sum(A[i]*B[j] for i in range(5) for j in range(7)) == sum(A)*sum(B)\n",
    "True\n",
    "```\n",
    "Note that the following does **not** hold\n",
    "\\begin{equation}\n",
    "\\sum_{i=0}^{m-1} a_i \\cdot a_i \\neq \\left(\\sum_{i=0}^{m-1} a_i\\right) \\cdot \\left(\\sum_{i=0}^{m-1} a_i\\right)\n",
    "\\end{equation}\n",
    "\n",
    "because this effectively treats the index $i$ in the first term independently from the $i$ in the second term of the product. Notice that in the right side of the equation, we could have interchanged the index $i$ in the second summation with $j$, without changing any of the mathematics. The sum on the left represents the summation of $m$ terms of $a_{i}^2$, whereas the sum on the right represents the summation of $m \\times m$ terms - products between all possible pairs of A's terms. \n",
    "\n",
    "```python\n",
    ">>> import numpy as np\n",
    ">>> A = np.random.rand(5)\n",
    ">>> sum(A[i]*A[i] for i in range(5)) == sum(A)*sum(A)\n",
    "False\n",
    "```\n",
    "\n",
    "Mathematicians are lazy. This means writing as little as possible to communicate, so note that typically when someone writes $\\sum_{i} x_i$, this is just the sum of all values in the sequence X, and is the same as writing $\\sum_{i=0}^{n-1} x_i$, where you'll usually know the value of n so you can still compute the sum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Matrices\n",
    "Suppose we have a m by n matrix that contains all products of the values in both sequence A and sequence B such that the matrix value of $M$ at index $(i,j)$ is $a_i \\cdot b_j$. \n",
    "\\begin{equation}\n",
    "M = \n",
    "\\begin{bmatrix}\n",
    "    a_1 \\cdot b_1 &  \\cdot & \\cdot & \\cdot & a_1 \\cdot b_n \\\\\n",
    "    \\cdot & \\cdot & &    & \\cdot \\\\\n",
    "    \\cdot & & \\cdot & & \\cdot \\\\\n",
    "    \\cdot & & & \\cdot & \\cdot \\\\\n",
    "    a_m \\cdot b_1 & \\cdot & \\cdot & \\cdot & a_m \\cdot b_n \n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "We can form this matrix in numpy via an \"outer product\":\n",
    "```python\n",
    ">>> import numpy as np\n",
    ">>> A = np.arange(5) # [0, 1, ..., 4]\n",
    ">>> B = np.arange(7) # [0, 1, ..., 6]\n",
    ">>> M = np.outer(A,B) # shape: (5, 7)\n",
    ">>> M\n",
    "array([[ 0,  0,  0,  0,  0,  0,  0],\n",
    "       [ 0,  1,  2,  3,  4,  5,  6],\n",
    "       [ 0,  2,  4,  6,  8, 10, 12],\n",
    "       [ 0,  3,  6,  9, 12, 15, 18],\n",
    "       [ 0,  4,  8, 12, 16, 20, 24]])\n",
    "```\n",
    "\n",
    "We can use this matrix to visualize several possible uses of summations. For example, suppose $m=n$. Then, if we express the sequences A and B as column vectors, the dot product of the two vectors would be the sum of the diagonal of M, or $\\sum_{k=0}^{n-1} a_k \\cdot b_k$. Furthermore, we can take the sum of a row using the sum $\\sum_{i=0}^{n-1} a_k \\cdot b_i$ for the $k^{th}$ row of the matrix, and similarly take the sum of a column with $\\sum_{i=0}^{n-1} a_i \\cdot b_k$ for the $k^{th}$ column of the matrix. \n",
    "\n",
    "```python\n",
    ">>> k = 2\n",
    "\n",
    "# sum over the kth column:\n",
    ">>> sum(M[:, k])\n",
    "20\n",
    "\n",
    "# sum over the kth row:\n",
    ">>> sum(M[k, :])\n",
    "42\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Kronecker Delta\n",
    "\n",
    "To make notation for working with summations (particularly in considering the matrix format) even simpler, we can use the Kronecker delta function, named after Prussian mathematician Leonard Kronecker. We use $\\delta_{i,j}$ to denote the Kronecker delta function, defined as:\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "  \\delta_{i, j}= 0 & \\text{if } i \\neq j\\\\    \n",
    "  \\delta_{i,j}=1 & \\text{if } i=j    \n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "See that a Kronecker-delta can \"collapse\" a sum. Let $j$ be an integer between 0 and $n-1$:\n",
    "\\begin{equation}\n",
    "\\sum_{i=0}^{n-1} \\delta_{i,j} a_i = 0 \\cdot a_0  + \\cdots + 1 \\cdot a_j + 0 \\cdot a_{j+1}+ \\cdots + 0 \\cdot  a_{n-1}= a_{j}\n",
    "\\end{equation}\n",
    "\n",
    "See also that the identity matrix, $I$, can be written as $I_{i,j}=\\delta_{i,j}$.\n",
    "\\begin{equation}\n",
    "I = \n",
    "\\begin{bmatrix}\n",
    "    1   & 0 & \\cdot & \\cdot & 0  \\\\\n",
    "    0 & 1 & 0 &  \\cdot  & \\cdot \\\\\n",
    "    \\cdot & 0 & 1 & 0 & \\cdot \\\\\n",
    "    \\cdot & \\cdot & 0 & 1 & 0 \\\\\n",
    "    0 & \\cdot & \\cdot & 0 & 1 \n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "As an exercise, write a Python function that behaves as a Kronecker delta, and include it in a for-loop that is computing a sum. Verify that it does indeed collapse the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Some Vector Calculus\n",
    "\n",
    "First, a quick review of partial derivatives. \n",
    "\n",
    "Partial derivatives are used in multivariable functions, in which we essentially derive with respect to one of these variables and treat the remaining variables as constants. For example:\n",
    "\\begin{equation}\n",
    "f(x,y) = 6 x^2 y^3\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial x} f(x,y) = 12x y^3\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial y} f(x,y) = 18 x^2 y^2 \n",
    "\\end{equation}\n",
    "\n",
    "So, what if we want to take a partial derivative of a sum? \n",
    "\n",
    "Suppose that we have two vectors containing variables: \n",
    " - $\\vec{x}=[x_0, x_1, \\cdots , x_j, \\cdots, x_{n-1}]$ \n",
    " - $\\vec{y} = [y_0, y_1, \\cdots, y_j, \\cdots, y_{n-1}]$. \n",
    " \n",
    "Then, in our previous section we saw that $\\vec{x} \\cdot \\vec{y} = \\sum_{i=0}^{n-1} x_i \\cdot y_i$. What happens when we take the partial derivative of this sum with respect to the variable $x_{j}$? Let $f = \\vec{x} \\cdot \\vec{y}$, then\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\sum_{i} x_i y_i = \\sum_i \\frac{\\partial x_i}{\\partial x_j} y_i\n",
    "\\end{equation}\n",
    "\n",
    "It is critical to see that, because $x_i$ and $x_j$ are distinct variables (unless $i = j$),\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "  \\frac{\\partial x_i}{\\partial x_j} = 0 & \\text{if } i \\neq j\\\\    \n",
    "  \\frac{\\partial x_i}{\\partial x_j} = 1 & \\text{if } i=j    \n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "and thus $\\frac{\\partial x_i}{\\partial x_j} = \\delta_{i,j}$. \n",
    "\n",
    "Thus we can simplify our sum by collapsing it via the Kronecker-delta:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\sum_{i} x_i y_i = \\sum_i \\frac{\\partial x_i}{\\partial x_j} y_i = \\sum_i \\delta_{i,j} y_i = y_j\n",
    "\\end{equation}\n",
    "\n",
    "Defining $\\frac{\\partial f}{\\partial \\vec{x}} = [\\frac{\\partial f}{\\partial x_0}, \\cdots , \\frac{\\partial f}{\\partial x_j}, \\cdots, \\frac{\\partial f}{\\partial x_{n-1}}]$, we can see from our above result that\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f}{\\partial \\vec{x}} = [y_0, \\cdots, y_j, \\cdots, y_{n-1}]\n",
    "\\end{equation}\n",
    "Take a little bit of time to think through the above expression, and make sure you understand why it is true, writing out matrices to help your understanding. This is just one example of how we can apply some of the calculus we already know to vectors, leading to vector calculus, a pillar for linear algebra! This is especially important for deriving expressions used in back-propagation in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Complex Numbers and Euler's Formula\n",
    "\n",
    "### 3.1 Complex Numbers\n",
    "\n",
    "Complex numbers are numbers that have both real and imaginary parts, and can be written in the form $a+bi$. We can think of complex numbers as geometric points, or coordinates on the complex plane (denoted as $\\mathbb{C}$), in which what we would normally label as the x-axis is the real axis, and what would be the y-axis is the complex axis. So, our number $a+bi$ is actually the point $(a,b)$ on the complex plane. Thinking of complex numbers as points in this complex plane is an important piece of intuition that will help us understand the important identity known as \"Euler's formula\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Euler's Formula \n",
    "Our work with Fourier series this summer will bring us in contact with an extremely important equation from the world of complex numbers. Euler's formula reveals a deep relationship between the exponential of a complex number and the cosine and sine functions:\n",
    "\n",
    "\\begin{equation}\n",
    "e^{i \\theta} = \\cos{\\theta} + i \\sin{\\theta}\n",
    "\\end{equation}\n",
    "\n",
    "Where we take $\\theta$ to be a real number $[\\theta \\in \\mathbb{R}]$ (this actually is not a necessary condition - it is only made for simplicity. This formula holds true even if $\\theta$ is itself a complex number). $\\theta$ is often referred to as the *phase* of the exponential. \n",
    "\n",
    "It is important to build some intuition for how $e^{i \\theta}$ behaves. Because $\\theta \\in \\mathbb{R}$, the real (cosine) and imaginary (sine) components of $e^{i \\theta}$ are both bounded between $[-1, 1]$. Furthermore, both components are **periodic** in $\\theta$ - they repeat themselves every time $\\theta$ increases by $2\\pi$. These properties of the sine and cosine functions should be familiar to you already.\n",
    "\n",
    "Using the following exercises as guidance, see that $\\cos{\\theta} + i \\sin{\\theta}$ can be represented as a point on the unit-circle (a circle with radius=1, depicted below), where $\\theta$ is the angle made with the positive real axis, in the direction of the positive imaginary-axis. At $\\theta = 0$, $e^{i \\theta} = e^{i 0} = \\cos{0} = 1$ resides at $(1 , i0)$. Increasing $\\theta$ causes this point to rotate *counter-clockwise* around the unit circle.\n",
    " \n",
    "Work through the following problems, and confirm your results by using `numpy.exp` and `numpy.pi`. Also, draw a complex plane, and **plot** the following results (where appropriate) on $\\mathbb{C}$.\n",
    "\n",
    " - Prove that $|e^{i \\theta}| = 1$.\n",
    " - Evaluate $e^{i 0}$\n",
    " - Evaluate $e^{i \\frac{\\pi}{2}}$\n",
    " - Evaluate $e^{i \\pi}$\n",
    " - Evaluate $e^{i \\frac{3\\pi}{2}}$\n",
    " - Evaluate $e^{i 2\\pi}$\n",
    " - Evaluate $e^{i n\\pi}$, where $n$ is any **odd** integer\n",
    " - Evaluate $e^{i n\\pi}$, where $n$ is any **even** integer\n",
    " - Using the fact that $e^{x + y} = e^{x}e^{y}$, prove that $e^{i (\\theta + 2n\\pi)} = e^{i \\theta}$, where $n$ is any integer\n",
    " \n",
    " ![title](pics/eulersformula.png)\n",
    "\n",
    "### 3.3 Applying Euler's Formula\n",
    "Euler's formula provides us with a convenient way of representing both cosine and sine functions. The arithmetic properties of exponentials (e.g. $e^{i\\theta_{1}}e^{i\\theta_{2}} = e^{i(\\theta_{1} + \\theta_{2})}$) are far simpler than those of the trigonometric functions. We will thus make heavy use of $e^{i\\theta}$ in lieu of $\\sin$ and $\\cos$ when we study Fourier series. \n",
    "\n",
    "Let's consider a concrete example of how Euler's formula can be used to represent cosine and sine functions. Remember those pesky trigonometric identities that you had to memorize for your math classes? For instance, \n",
    "\\begin{equation}\n",
    "\\sin(\\alpha + \\beta) = \\sin\\alpha\\cos\\beta + \\cos\\alpha\\sin\\beta\\\\\n",
    "\\cos(\\alpha + \\beta) = \\cos\\alpha\\sin\\beta + \\cos\\alpha\\sin\\beta\n",
    "\\end{equation}\n",
    "\n",
    "They are easily derived using Euler's formula:\n",
    "\n",
    "\\begin{equation}\n",
    "e^{i(\\alpha + \\beta)} = \\cos(\\alpha + \\beta) + i\\sin(\\alpha + \\beta)\\\\\n",
    "\\end{equation}\n",
    "\n",
    "$e^{i(\\alpha + \\beta)}$ can also be rewritten as the product of two exponentials, providing us with a second expression:\n",
    "\\begin{align*}\n",
    "e^{i(\\alpha + \\beta)} &= e^{i\\alpha}e^{i\\beta} \\\\\n",
    "&= (\\cos\\alpha + i\\sin\\alpha)(\\cos\\beta + i\\sin\\beta) \\\\\n",
    "&= \\cos\\alpha\\cos\\beta - \\sin\\alpha\\sin\\beta + i(\\cos\\alpha\\sin\\beta + \\sin\\alpha\\cos\\beta)\n",
    "\\end{align*}\n",
    "\n",
    "Thus \n",
    "\\begin{equation}\n",
    "\\cos(\\alpha + \\beta) + i\\sin(\\alpha + \\beta) = (\\cos\\alpha\\cos\\beta - \\sin\\alpha\\sin\\beta) + i(\\cos\\alpha\\sin\\beta + \\sin\\alpha\\cos\\beta)\n",
    "\\end{equation}\n",
    "\n",
    "In general, complex numbers satisfy the equality relationship that if $a + ib = c + id$, then $a = c$ and $b = d$. Thus we can conclude that:\n",
    "\\begin{equation}\n",
    "\\cos(\\alpha + \\beta) = \\cos\\alpha\\cos\\beta - \\sin\\alpha\\sin\\beta\\\\\n",
    "\\sin(\\alpha + \\beta) = \\cos\\alpha\\sin\\beta + \\sin\\alpha\\cos\\beta\n",
    "\\end{equation}\n",
    "\n",
    "Using Euler's formula allows us to trivially derive these two angle identities! You no longer need to waste time memorizing them!\n",
    "\n",
    "Euler's formula appears throughout areas that deal with oscillitory/wave phenomena (e.g. electricity & magnetism, classical mechanics, quantum mechanics, circuits, acoustics, ...)\n",
    "\n",
    "#### Exercises\n",
    " - Prove that $\\cos(\\theta) = \\frac{e^{i\\theta} + e^{-i\\theta}}{2}$\n",
    " - Prove that $\\sin(\\theta) = \\frac{e^{i\\theta} - e^{-i\\theta}}{2i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justifying Euler's Formula (OPTIONAL)\n",
    "There are several ways in which we can justify this expression. The most common justification uses the MacLaurin series (i.e. a Taylor series centered at $x = 0$)\n",
    "\\begin{equation}\n",
    "e^x = 1 + \\frac{x}{1!} + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!} + \\cdots\n",
    "\\end{equation}\n",
    "Then, when we substitute in $x = i \\theta$\n",
    "\\begin{align*}\n",
    "e^{i \\theta} &= 1 + i \\theta - \\frac{\\theta^2}{2!} - i \\frac{\\theta^3}{3!}+\\frac{\\theta^4}{4!}+ i\\frac{\\theta^5}{5!} + \\cdots \\\\\n",
    "&=(1-\\frac{\\theta^2}{2!}+ \\frac{\\theta^4}{4!}-\\frac{\\theta^6}{6!}+ \\cdots) + i(\\theta - \\frac{\\theta^3}{3!} + \\frac{\\theta^5}{5!}-\\frac{\\theta^7}{7!} + \\cdots) \\\\\n",
    "&=\\cos{\\theta} + i \\sin{\\theta}\n",
    "\\end{align*}\n",
    "\n",
    "We can also explore a proof that does not require knowledge of specific MacLaurin series.\n",
    "\n",
    "Consider the equation $f(t) = e^{-it}(\\cos{t}+i\\sin{t}) \\forall t \\in \\mathbb{R}$. Then, by the product rule, we find that \n",
    "\\begin{align*}\n",
    "f'(t) &= e^{-it}(i\\cos{t}-\\sin{t})-ie^{-it}(\\cos{t}+i\\sin{t}) \\\\\n",
    "&= e^{-it}(i\\cos{t}-\\sin{t})-e^{-it}(i\\cos{t}-\\sin{t})=0 \\forall t \\in \\mathbb{R}\n",
    "\\end{align*}\n",
    "\n",
    "Because $f'(t) = 0$, $f(t)$ must be constant. Furthermore, because $f(0)=1$, then it must hold that $f(t)=1$ for all values of $t$. \n",
    "\\begin{align*}\n",
    "f(t) = e^{-it}(\\cos{t}+i\\sin{t}) &= 1 \\\\\n",
    "\\implies\\; e^{it} &= (\\cos{t}+i\\sin{t})\n",
    "\\end{align*}\n",
    "\n",
    "Final note:\n",
    "\n",
    "To be completely thorough, some care needs to be taken to formalize a definition of what a derivative is for a complex-valued function, and furthermore to justify that $\\frac{d(e^{ix})}{dx} = ie^{ix}$. Thus we should not really call this a proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Interative Introduction to Fourier Transforms\n",
    "> Material from Jez Swanson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carefully read through the [An Interactive Introduction to Fourier Transforms](http://www.jezzamon.com/fourier/) by Jez Swanson. Once you have gone through it, complete the questions below.\n",
    "\n",
    "##### What does a Fourier transform do?\n",
    "> Solution: *SOLUTION HERE*\n",
    "\n",
    "##### How many sine waves are required to construct a square wave perfectly?\n",
    "> Solution: *SOLUTION HERE*\n",
    "\n",
    "##### How do Fourier transforms allow us to compress media?\n",
    "> Solution: *SOLUTION HERE*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
